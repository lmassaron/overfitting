{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting problem: p ~ n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main objectives of predictive modelling is to build a model that will give accurate predictions on unseen data.\n",
    "\n",
    "A necessary step in the building of models is to ensure that they have not overfit the training data, which leads to sub optimal predictions on new data.\n",
    "\n",
    "The purpose of this challenge is to stimulate research and highlight existing algorithms, techniques or strategies that can be used to guard against overfitting.\n",
    "\n",
    "In order to achieve this we have created a simulated data set with 200 variables and 20,000 cases. An ‘equation’ based on this data was created in order to generate a Target to be predicted. Given the all 20,000 cases, the problem is very easy to solve – but you only get given the Target value of 250 cases – the task is to build a model that gives the best predictions on the remaining 19,750 cases.\n",
    "\n",
    "This competition is of particular relevance to medical data analysis, where often the number of cases is severely restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './overfitting.csv'\n",
    "dataset = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.loc[dataset.train==1]\n",
    "test = dataset.loc[dataset.train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Target_Practice.values\n",
    "X = train.iloc[:,5:].copy()\n",
    "Xt = test.iloc[:,5:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values: [0 1]\n",
      "Target distribution: [119 131]\n"
     ]
    }
   ],
   "source": [
    "values, counts = np.unique(y, return_counts=True)\n",
    "print('Target values: {}'.format(values))\n",
    "print('Target distribution: {}'.format(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (250, 200)\n",
      "Test set size: (19750, 200)\n"
     ]
    }
   ],
   "source": [
    "print ('Training set size: {}'.format(X.shape))\n",
    "print ('Test set size: {}'.format(Xt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', probability=True)\n",
    "params = {'C': np.logspace(-7, 3, 100), \n",
    "          'gamma': np.logspace(-7, 2, 100)}\n",
    "\n",
    "rand = RandomizedSearchCV(estimator=clf, \n",
    "                          param_distributions=params,\n",
    "                          n_iter=100,\n",
    "                          scoring='roc_auc',\n",
    "                          n_jobs=-1,\n",
    "                          cv=10,\n",
    "                          refit=True,\n",
    "                          random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.fit(X, y)\n",
    "best_roc_auc = rand.best_score_\n",
    "best_params = rand.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc 0.859 using {'gamma': 0.005336699231206312, 'C': 61.35907273413163}\n"
     ]
    }
   ],
   "source": [
    "print('Best roc_auc {:1.3f} using {}'.format(best_roc_auc, best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem. The AUC on the test portion will be used to evaluate the benchmark model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rand.predict_proba(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc in the test set is 0.862\n"
     ]
    }
   ],
   "source": [
    "test_roc_auc = roc_auc_score(y_true=test.Target_Practice.values, \n",
    "                             y_score=probs[:,1])\n",
    "\n",
    "print('roc_auc in the test set is {:1.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement of the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to propose a better model than the benchmark. An AUC above 0.90 is considered as a good result. AUC scores about 0.95 are considered excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
